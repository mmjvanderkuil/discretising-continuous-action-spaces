{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import gym\n",
    "import gymnasium\n",
    "\n",
    "from TD3.ContinuousCartPole import *\n",
    "from TD3.ContinuousPendulum import *\n",
    "from TD3.ContinuousMountainCar import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Evaluate the trained agent\n",
    "# env = gym.make(\"MountainCarContinuous-v0\")\n",
    "# env= gymnasium.make(\"MountainCarContinuous-v0\")\n",
    "env = MountainCarContinuousEnv()\n",
    "\n",
    "# model = TD3.load(\"models/td3-ContinuousMountainCar.zip\", env=env)\n",
    "model = TD3.load(\"models/pretrained/MountainCarContinuous-v0.zip\", env=env)\n",
    "print(\"\\n=== Continuous Mountain Car TD3 Evaluation ===\")\n",
    "mcc_actions = []\n",
    "for ep in range(10):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    trunc = False\n",
    "    total_reward = 0.0\n",
    "    actions = []\n",
    "    i = 0\n",
    "    velocities = []\n",
    "    while not done and not trunc:\n",
    "        i+= 1\n",
    "        velocities.append(obs[1])\n",
    "        action, _ = model.predict(obs, deterministic=False)\n",
    "        actions.append(float(action))\n",
    "        obs, reward, done, trunc, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "    mcc_actions.append(actions)\n",
    "    print(f\"Episode {ep+1}: total_reward={total_reward:.8f}, actions={actions[:10]}\")"
   ],
   "id": "1aaf180eef16fab6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Evaluate the trained agent\n",
    "env = PendulumEnv()\n",
    "model = TD3.load(\"models/td3-ContinuousPendulum.zip\", env=env)\n",
    "print(\"\\n=== Continuous Pendulum TD3 Evaluation ===\")\n",
    "penc_actions = []\n",
    "for ep in range(10):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    trunc = False\n",
    "    total_reward = 0.0\n",
    "    actions = []\n",
    "    while not done and not trunc:\n",
    "        # print(\"Hello\")\n",
    "        action, _ = model.predict(obs, deterministic=True,)\n",
    "        actions.append(float(action))\n",
    "        obs, reward, done, trunc, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "    penc_actions.append(actions)\n",
    "    print(f\"Episode {ep+1}: total_reward={total_reward:.2f}, actions={actions[:10]}, length={len(actions)}\")\n"
   ],
   "id": "b89e9768d51fdd7f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Evaluate the trained agent\n",
    "env = ContinuousCartPoleEnv()\n",
    "model = TD3.load(\"models/td3-ContinuousCartPole\", env=env)\n",
    "print(\"\\n=== Continuous CartPole TD3 Evaluation ===\")\n",
    "cpc_actions = []\n",
    "for ep in range(10):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    trunc = False\n",
    "    total_reward = 0.0\n",
    "    actions = []\n",
    "    while not done and not trunc:\n",
    "        action, _ = model.predict(obs, deterministic=False)\n",
    "        actions.append(float(action))\n",
    "        obs, reward, done, trunc, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "    cpc_actions.append(actions)\n",
    "    \n",
    "    print(f\"Episode {ep+1}: total_reward={total_reward:.2f}, actions={actions[:10]}\")"
   ],
   "id": "1e53add8d60a0d59",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "flat = []\n",
    "all_actions = mcc_actions\n",
    "for x in all_actions:\n",
    "    for i in x : \n",
    "        flat.append(i)\n",
    "# print(len(flat))\n",
    "data = np.array(flat)\n",
    "\n",
    "bins = 100\n",
    "counts = [0] * bins # create array of zeros\n",
    "bin_edges = [0] * (bins + 1)\n",
    "# YOUR CODE HERE\n",
    "total = 100\n",
    "max_value = max(data)\n",
    "min_value = min(data)\n",
    "\n",
    "bin_edges = np.linspace(min_value, max_value, bins + 1)\n",
    "s_data = sorted(data)\n",
    "i = 0\n",
    "for val in s_data:\n",
    "    if val <= bin_edges[i+1]:\n",
    "        counts[i] += 1\n",
    "    else:\n",
    "        i += 1\n",
    "        counts[i] += 1"
   ],
   "id": "87477d25c3c9ce9d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.hist(flat)\n",
    "plt.plot()"
   ],
   "id": "3d83d8c1f48c00f6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x = []\n",
    "y = []\n",
    "for _y, _x in enumerate(s_data):\n",
    "    x.append(_x)\n",
    "    y.append(_y/len(s_data))\n",
    "\n",
    "# print(len(data))\n",
    "# print(len(y))\n",
    "\n",
    "for q in np.arange(0,1,0.1):\n",
    "    print(np.quantile(s_data, q=q))\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ],
   "id": "d7e148a1fae54fc4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "K = 8 # number of actions\n",
    "diff = np.inf \n",
    "sd_dif = np.inf\n",
    "sample_mean = data.mean()\n",
    "sample_std = data.std()\n",
    "\n",
    "result_Acts = []\n",
    "for _ in range(5):\n",
    "    # Sample until representable\n",
    "    actions = []\n",
    "    while abs(diff) > 0.25 or abs(sd_dif) > 0.05:\n",
    "        actions = random.choices(data, k = K)\n",
    "        diff = np.mean(actions) - sample_mean\n",
    "        sd_dif = np.std(actions) - sample_std\n",
    "    result_Acts.append(actions)\n",
    "    diff = np.inf\n",
    "    sd_dif = np.inf"
   ],
   "id": "3bc0c4d8990ed2c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "[print(str(x)) for x in result_Acts]",
   "id": "755939316d2de8a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "run_cpc = \"echo 'exp2b cp {}' \\n./target/release/broccoli --env cpc --depth 2 --num-nodes 3 --num-iters 10000 --predicate-increment 0.1 0.1 0.05 0.1 --initial-state-values -0.05 0.05 0.05 0.05 --predicate-reasoning 1 --actions {} > TD3_acts_cpc_s={}.txt\"\n",
    "run_penc = \"echo 'exp2a penc {}' \\n./target/release/broccoli --env penc --depth 2 --num-nodes 3 --num-iters 100  --predicate-increment 0.1 0.1 --initial-state-values 0.5 0 --predicate-reasoning 1 --actions {}  > TD3_acts_penc_s={}.txt\"\n",
    "run_mcc = \"echo 'exp2b mcc {}' \\n./target/release/broccoli --env mcc --depth 2 --num-nodes 3 --num-iters 1000 --predicate-increment 0.1 0.014 --initial-state-values -0.5 0.0 --predicate-reasoning 1 --actions {} > TD3_acts_mcc_s={}.txt\"\n",
    "\n",
    "run = run_cpc\n",
    "with open(\"action_exp/exp4_cpc.sh\", \"w\") as f:\n",
    "    for i, x in enumerate(result_Acts):\n",
    "        x.sort()\n",
    "        string = \"\"\n",
    "        for y in x:\n",
    "            string += \"{:.6f} \".format(y)\n",
    "        print(string)\n",
    "        f.write(run.format(i,string, i))\n",
    "        f.write(\"\\n\")"
   ],
   "id": "f7346a0d51f5b2e2",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
